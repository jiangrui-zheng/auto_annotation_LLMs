{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ee6efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "CUDA SETUP: Loading binary /data/installation/anaconda3/envs/lora/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/installation/anaconda3/envs/lora/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: /data/installation/anaconda3/envs/lora did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/data/installation/anaconda3/envs/lora/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:147: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d05551c0cc47d5ab1b384b1911a0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from transformers import (\n",
    "    GPTJForCausalLM,\n",
    "    GPT2Tokenizer,\n",
    "    LlamaForCausalLM, \n",
    "    LlamaTokenizer,\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import transformers\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# model_path = \"/data/jzheng36/Fine-Tune-Llama2-/llama2_ft_7B-lr=2e-4\"\n",
    "model_path = \"/data/shared/llama2/llama/7B-Chat/\"\n",
    "# model_path = \"philschmid/llama-2-7b-instruction-generator\"\n",
    "\n",
    "# model_path = \"/data/jzheng36/model/instruct_gpt4chan_3epoch\"\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# # GPT-4chan\n",
    "# model = GPTJForCausalLM.from_pretrained(\n",
    "#     get_last_checkpoint(model_path) # 3, 10\n",
    "# ).to(device)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    device=0,\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99759a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pathlib\n",
    "\n",
    "from termcolor import (\n",
    "    cprint,\n",
    "    colored\n",
    ")\n",
    "\n",
    "def extract_category(text):\n",
    "    patterns = [\n",
    "        r'Category: \\[(.*?)\\]',\n",
    "        r\"\\\\n \\\\n Category:(.*?) \\\\n \\\\n\",\n",
    "        r\"\\n \\n Category:(.*?) \\n\",\n",
    "        r'\\n Category:(.*?)\\\"',\n",
    "        r'\\n Category:(.*?)$'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2943e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('guidelines.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abe58a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_by_twitter(text):\n",
    "    examples = [\n",
    "        {\n",
    "            \"label\": \"Non-hate\",\n",
    "            \"index\": 41,\n",
    "            \"text\": \"Asian are good\\nWe should help trans\",\n",
    "            \"description\": \"Other non-hateful sentences.\"\n",
    "        },\n",
    "    ]\n",
    "    for i in range(41):\n",
    "        example = {\n",
    "                \"label\": df[\"guideline\"][i],\n",
    "                \"index\": df[\"index\"][i],\n",
    "                \"text\": df[\"examples\"][i],\n",
    "                \"description\": df[\"description\"][i]\n",
    "            }\n",
    "        examples.append(example)\n",
    "\n",
    "\n",
    "    prefix = \"Classify the sentence into one of the labels and response by index only:\\n\"\n",
    "#     prompt_template = \"\\n\\n\".join(\"- text: {}\\n- label: {}\".format(example[\"text\"], example[\"label\"]) \n",
    "#                                   for example in random.sample(examples, k=len(examples)))\n",
    "#     prompt_template = \"\\n\\n\".join(\"- text: {}\\n- label: {}\\n- description: {}\".format(example[\"text\"], example[\"label\"], example[\"description\"]) \n",
    "#                               for example in random.sample(examples, k=len(examples)))\n",
    "    prompt_template = \"\\n\\n\".join(\"- label: {}\\n- description: {}\".format(example[\"label\"], example[\"description\"]) \n",
    "                              for example in random.sample(examples, k=len(examples)))\n",
    "    suffix = \"\\n\\n- text: {}\\n- label:\".format(text)\n",
    "\n",
    "\n",
    "    text_input = prefix + prompt_template + suffix\n",
    "    return text_input\n",
    "    \n",
    "text = \"Donald Trump is smelly pig.\"\n",
    "text_input = classify_by_twitter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b611931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the sentence into one of the labels and response by index only:\n",
      "- label: segregation\n",
      "- description: Segregation in the form of calls for action, statements of intent, aspirational or conditional statements, or statements advocating or supporting segregation.\n",
      "\n",
      "- label: attacking character trait\n",
      "- description: Attacking a person or group based on a perceived character trait , Content attacking a person or a group based on their character traits culturally perceived as negative, including but not limited to: coward, liar, arrogant, ignorant.\n",
      "\n",
      "- label: attacking appearance\n",
      "- description: Content attacking a person or a group based on their physical appearance, including but not limited to: ugly, hideous.\n",
      "\n",
      "- label: attacking education\n",
      "- description: Content attacking a person or a group based on their education, including but not limited to: illiterate, uneducated.\n",
      "\n",
      "- label: bacteria\n",
      "- description: Dehumanizing speech: Bacteria, viruses, or microbes\n",
      "\n",
      "- label: contempt despise dislike\n",
      "- description: Expressions of dismissal, including but not limited to: don´t respect, don't like, don´t care for\n",
      "\n",
      "- label: dehumanization animal\n",
      "- description: Dehumanizing speech: Animals in general or specific types of animals that are culturally perceived as intellectually or physically inferior (including but not limited to: Black people and apes or ape-like\n",
      "\n",
      "- label: filth\n",
      "- description: Dehumanizing speech: Filth (including but not limited to: dirt, grime)\n",
      "\n",
      "- label: disgust repulsion\n",
      "- description: Expressions of repulsion or distaste, including but not limited to: vile, disgusting, yuck.\n",
      "\n",
      "- label: curse genitalia\n",
      "- description: Curse that referring to the target as genitalia or anus, including but not limited to: cunt, dick, asshole.\n",
      "\n",
      "- label: attacking hygiene\n",
      "- description: Content attacking a person or a group based on their hygiene, including but not limited to: filthy, dirty, smelly.\n",
      "\n",
      "- label: attack concept associated protected characteristics\n",
      "- description: Content attacking concepts, institutions, ideas, practices, or beliefs associated with protected characteristics, which are likely to contribute to imminent physical harm, intimidation or discrimination against the people associated with that protected characteristic.\n",
      "\n",
      "- label: less than adequate\n",
      "- description: Expressions about being less than adequate, including but not limited to: worthless, useless\n",
      "\n",
      "- label: hatecrime victim\n",
      "- description: Mocking the concept, events or victims of hate crimes even if no real person is depicted in an image.\n",
      "\n",
      "- label: attacking derogatory term\n",
      "- description: Content attacking a person or a group based on derogatory terms related to sexual activity, including but not limited to: whore, slut, perverts.\n",
      "\n",
      "- label: explicit exclusion\n",
      "- description: Call for action of exclusion, e.g., explicit exclusion, which means things like expelling certain groups or saying they are not allowed.\n",
      "\n",
      "- label: change sexual\n",
      "- description: Content explicitly providing or offering to provide products or services that aim to change people’s sexual orientation or gender identity.\n",
      "\n",
      "- label: economic exclusion\n",
      "- description: Call for action of exclusion, e.g., economic exclusion, which means denying access to economic entitlements and limiting participation in the labour market.\n",
      "\n",
      "- label: deviating norm\n",
      "- description: Expressions about deviating from the norm, including but not limited to: freaks, abnormal.\n",
      "\n",
      "- label: feces\n",
      "- description: Dehumanizing speech: Feces (including but not limited to: shit, crap)\n",
      "\n",
      "- label: spread virus\n",
      "- description: Content targeting a person or group of people on the basis of their protected characteristic(s) with claims that they have or spread the novel coronavirus, are responsible for the existence of the novel coronavirus, are deliberately spreading the novel coronavirus, or mocking them for having or experiencing the novel coronavirus.\n",
      "\n",
      "- label: attacking intellectual capability\n",
      "- description: Content attacking a person or a group based on their intellectual capacity, including but not limited to: dumb, stupid, idiots.\n",
      "\n",
      "- label: disease\n",
      "- description: Dehumanizing speech: Disease (including but not limited to: cancer, sexually transmitted diseases)\n",
      "\n",
      "- label: contempt self admission intolerance\n",
      "- description: Self-admission to intolerance on the basis of a protected characteristics, including but not limited to: homophobic, islamophobic, racist.\n",
      "\n",
      "- label: better worse than\n",
      "- description: Expressions about being better/worse than another protected characteristic, including but not limited to: \"I believe that males are superior to females.\"\n",
      "\n",
      "- label: Non-hate\n",
      "- description: Other non-hateful sentences.\n",
      "\n",
      "- label: sexual predator\n",
      "- description: Dehumanizing speech: Sexual predators (including but not limited to: Muslim people having sex with goats or pigs)\n",
      "\n",
      "- label: political exclusion\n",
      "- description: Call for action of exclusion, e.g., political exclusion, which means denying the right to political participation.\n",
      "\n",
      "- label: attacking mental health\n",
      "- description: Content attacking a person or a group based on their mental health, including but not limited to: mentally ill, retarded, crazy, insane.\n",
      "\n",
      "- label: disgust vomit\n",
      "- description: Expressions that suggest the target causes sickness, including but not limited to: vomit, throw up.\n",
      "\n",
      "- label: subhumanity\n",
      "- description: Dehumanizing speech: Subhumanity (including but not limited to: savages, devils, monsters, primitives)\n",
      "\n",
      "- label: harmful stereotype\n",
      "- description: Harmful stereotypes historically linked to intimidation, exclusion, or violence on the basis of a protected characteristic, such as Blackface; Holocaust denial; claims that Jewish people control financial, political, or media institutions; and references to Dalits as menial laborers\n",
      "\n",
      "- label: contempt despise hate\n",
      "- description: Expressions of hate, including but not limited to: despise, hate.\n",
      "\n",
      "- label: violent speech\n",
      "- description: Violent speech or support in written or visual form\n",
      "\n",
      "- label: social exclusion\n",
      "- description: Call for action of exclusion, e.g., social exclusion, which means things like denying access to spaces (physical and online)and social services, except for gender-based exclusion in health and positive support Groups.\n",
      "\n",
      "- label: deny existence\n",
      "- description: Statements denying existence (including but not limited to: \"[protected characteristic(s) or quasi-protected characteristic] do not exist\", \"no such thing as [protected charactic(s) or quasi-protected characteristic]\" ), deny existence is different from contempt-should-not-exist in tier 2\n",
      "\n",
      "- label: insects\n",
      "- description: Dehumanizing speech: Insects (including but not limited to: cockroaches, locusts)\n",
      "\n",
      "- label: certain objects\n",
      "- description: Certain objects (women as household objects or property or objects in general; Black people as farm equipment; transgender or non-binary people as “it”)\n",
      "\n",
      "- label: curse sexual\n",
      "- description: Terms or phrases calling for engagement in sexual activity, or contact with genitalia, anus, feces or urine, including but not limited to: suck my dick, kiss my ass, eat shit.\n",
      "\n",
      "- label: contempt shouldnt exist\n",
      "- description: Expressions that a protected characteristic shouldn't exist. (shouldn't exist is different from deny-existence in tier 1)\n",
      "\n",
      "- label: curse profane\n",
      "- description: Profane terms or phrases with the intent to insult, including but not limited to: fuck, bitch, motherfucker.\n",
      "\n",
      "- label: criminal\n",
      "- description: Violent criminals (including but not limited to: terrorists, murderers, members of hate or criminal organizations)，Other criminals (including but not limited to “thieves,” “bank robbers,” or saying “All [protected characteristic or quasi-protected characteristic] are ‘criminals’”).\n",
      "\n",
      "- text: Donald Trump is smelly pig.\n",
      "- label:\n"
     ]
    }
   ],
   "source": [
    "print(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fc54319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mClassify the sentence into one of the labels and response by index only:\n",
      "- index: 14\n",
      "- label: attacking hygiene\n",
      "- description: Content attacking a person or a group based on their hygiene, including but not limited to: filthy, dirty, smelly.\n",
      "\n",
      "- index: 16\n",
      "- label: attacking intellectual capability\n",
      "- description: Content attacking a person or a group based on their intellectual capacity, including but not limited to: dumb, stupid, idiots.\n",
      "\n",
      "- index: 15\n",
      "- label: attacking appearance\n",
      "- description: Content attacking a person or a group based on their physical appearance, including but not limited to: ugly, hideous.\n",
      "\n",
      "- index: 8\n",
      "- label: subhumanity\n",
      "- description: Dehumanizing speech: Subhumanity (including but not limited to: savages, devils, monsters, primitives)\n",
      "\n",
      "- index: 6\n",
      "- label: feces\n",
      "- description: Dehumanizing speech: Feces (including but not limited to: shit, crap)\n",
      "\n",
      "- index: 32\n",
      "- label: curse profane\n",
      "- description: Profane terms or phrases with the intent to insult, including but not limited to: fuck, bitch, motherfucker.\n",
      "\n",
      "- index: 18\n",
      "- label: attacking mental health\n",
      "- description: Content attacking a person or a group based on their mental health, including but not limited to: mentally ill, retarded, crazy, insane.\n",
      "\n",
      "- index: 13\n",
      "- label: hatecrime victim\n",
      "- description: Mocking the concept, events or victims of hate crimes even if no real person is depicted in an image.\n",
      "\n",
      "- index: 20\n",
      "- label: attacking derogatory term\n",
      "- description: Content attacking a person or a group based on derogatory terms related to sexual activity, including but not limited to: whore, slut, perverts.\n",
      "\n",
      "- index: 19\n",
      "- label: attacking character trait\n",
      "- description: Attacking a person or group based on a perceived character trait , Content attacking a person or a group based on their character traits culturally perceived as negative, including but not limited to: coward, liar, arrogant, ignorant.\n",
      "\n",
      "- index: 11\n",
      "- label: deny existence\n",
      "- description: Statements denying existence (including but not limited to: \"[protected characteristic(s) or quasi-protected characteristic] do not exist\", \"no such thing as [protected charactic(s) or quasi-protected characteristic]\" ), deny existence is different from contempt-should-not-exist in tier 2\n",
      "\n",
      "- index: 35\n",
      "- label: political exclusion\n",
      "- description: Call for action of exclusion, e.g., political exclusion, which means denying the right to political participation.\n",
      "\n",
      "- index: 3\n",
      "- label: bacteria\n",
      "- description: Dehumanizing speech: Bacteria, viruses, or microbes\n",
      "\n",
      "- index: 10\n",
      "- label: certain objects\n",
      "- description: Certain objects (women as household objects or property or objects in general; Black people as farm equipment; transgender or non-binary people as “it”)\n",
      "\n",
      "- index: 7\n",
      "- label: sexual predator\n",
      "- description: Dehumanizing speech: Sexual predators (including but not limited to: Muslim people having sex with goats or pigs)\n",
      "\n",
      "- index: 12\n",
      "- label: harmful stereotype\n",
      "- description: Harmful stereotypes historically linked to intimidation, exclusion, or violence on the basis of a protected characteristic, such as Blackface; Holocaust denial; claims that Jewish people control financial, political, or media institutions; and references to Dalits as menial laborers\n",
      "\n",
      "- index: 36\n",
      "- label: economic exclusion\n",
      "- description: Call for action of exclusion, e.g., economic exclusion, which means denying access to economic entitlements and limiting participation in the labour market.\n",
      "\n",
      "- index: 4\n",
      "- label: disease\n",
      "- description: Dehumanizing speech: Disease (including but not limited to: cancer, sexually transmitted diseases)\n",
      "\n",
      "- index: 26\n",
      "- label: contempt despise hate\n",
      "- description: Expressions of hate, including but not limited to: despise, hate.\n",
      "\n",
      "- index: 41\n",
      "- label: Non-hate\n",
      "- description: Other non-hateful sentences.\n",
      "\n",
      "- index: 17\n",
      "- label: attacking education\n",
      "- description: Content attacking a person or a group based on their education, including but not limited to: illiterate, uneducated.\n",
      "\n",
      "- index: 21\n",
      "- label: less than adequate\n",
      "- description: Expressions about being less than adequate, including but not limited to: worthless, useless\n",
      "\n",
      "- index: 23\n",
      "- label: better worse than\n",
      "- description: Expressions about being better/worse than another protected characteristic, including but not limited to: \"I believe that males are superior to females.\"\n",
      "\n",
      "- index: 39\n",
      "- label: attack concept associated protected characteristics\n",
      "- description: Content attacking concepts, institutions, ideas, practices, or beliefs associated with protected characteristics, which are likely to contribute to imminent physical harm, intimidation or discrimination against the people associated with that protected characteristic.\n",
      "\n",
      "- index: 25\n",
      "- label: contempt shouldnt exist\n",
      "- description: Expressions that a protected characteristic shouldn't exist. (shouldn't exist is different from deny-existence in tier 1)\n",
      "\n",
      "- index: 22\n",
      "- label: deviating norm\n",
      "- description: Expressions about deviating from the norm, including but not limited to: freaks, abnormal.\n",
      "\n",
      "- index: 28\n",
      "- label: curse sexual\n",
      "- description: Terms or phrases calling for engagement in sexual activity, or contact with genitalia, anus, feces or urine, including but not limited to: suck my dick, kiss my ass, eat shit.\n",
      "\n",
      "- index: 40\n",
      "- label: spread virus\n",
      "- description: Content targeting a person or group of people on the basis of their protected characteristic(s) with claims that they have or spread the novel coronavirus, are responsible for the existence of the novel coronavirus, are deliberately spreading the novel coronavirus, or mocking them for having or experiencing the novel coronavirus.\n",
      "\n",
      "- index: 1\n",
      "- label: violent speech\n",
      "- description: Violent speech or support in written or visual form\n",
      "\n",
      "- index: 24\n",
      "- label: contempt self admission intolerance\n",
      "- description: Self-admission to intolerance on the basis of a protected characteristics, including but not limited to: homophobic, islamophobic, racist.\n",
      "\n",
      "- index: 34\n",
      "- label: explicit exclusion\n",
      "- description: Call for action of exclusion, e.g., explicit exclusion, which means things like expelling certain groups or saying they are not allowed.\n",
      "\n",
      "- index: 5\n",
      "- label: dehumanization animal\n",
      "- description: Dehumanizing speech: Animals in general or specific types of animals that are culturally perceived as intellectually or physically inferior (including but not limited to: Black people and apes or ape-like\n",
      "\n",
      "- index: 9\n",
      "- label: criminal\n",
      "- description: Violent criminals (including but not limited to: terrorists, murderers, members of hate or criminal organizations)，Other criminals (including but not limited to “thieves,” “bank robbers,” or saying “All [protected characteristic or quasi-protected characteristic] are ‘criminals’”).\n",
      "\n",
      "- index: 30\n",
      "- label: disgust repulsion\n",
      "- description: Expressions of repulsion or distaste, including but not limited to: vile, disgusting, yuck.\n",
      "\n",
      "- index: 29\n",
      "- label: disgust vomit\n",
      "- description: Expressions that suggest the target causes sickness, including but not limited to: vomit, throw up.\n",
      "\n",
      "- index: 38\n",
      "- label: change sexual\n",
      "- description: Content explicitly providing or offering to provide products or services that aim to change people’s sexual orientation or gender identity.\n",
      "\n",
      "- index: 37\n",
      "- label: social exclusion\n",
      "- description: Call for action of exclusion, e.g., social exclusion, which means things like denying access to spaces (physical and online)and social services, except for gender-based exclusion in health and positive support Groups.\n",
      "\n",
      "- index: 2\n",
      "- label: insects\n",
      "- description: Dehumanizing speech: Insects (including but not limited to: cockroaches, locusts)\n",
      "\n",
      "- index: 0\n",
      "- label: filth\n",
      "- description: Dehumanizing speech: Filth (including but not limited to: dirt, grime)\n",
      "\n",
      "- index: 31\n",
      "- label: curse genitalia\n",
      "- description: Curse that referring to the target as genitalia or anus, including but not limited to: cunt, dick, asshole.\n",
      "\n",
      "- index: 33\n",
      "- label: segregation\n",
      "- description: Segregation in the form of calls for action, statements of intent, aspirational or conditional statements, or statements advocating or supporting segregation.\n",
      "\n",
      "- index: 27\n",
      "- label: contempt despise dislike\n",
      "- description: Expressions of dismissal, including but not limited to: don´t respect, don't like, don´t care for\n",
      "\n",
      "- text: Donald Trump is smelly pig.\n",
      "- label:\u001b[0m\u001b[32m attacking hygiene\n",
      "- description: Content attacking a person or a group based on their hygiene, including but not limited to: filthy, dirty, smelly.\n",
      "\n",
      "- text: Black people are less than human.\n",
      "- label: dehumanization\n",
      "- description: Dehumanizing speech: Black people (including but not limited to: subhuman, less than human, savages)\n",
      "\n",
      "- text: Muslims are terrorists.\n",
      "- label: hatecrime victim\n",
      "- description: Mocking the concept, events or victims of hate crimes even if no real person is depicted in an image.\n",
      "\n",
      "- text: Women are inferior to men.\n",
      "- label: harmful stereotype\n",
      "- description: Harmful stereotypes historically linked to intimidation, exclusion, or violence on the basis of a protected characteristic, such as Blackface; Holocaust denial; claims that Jewish\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = pipeline(text_input)\n",
    "continuation = response[0]['generated_text'].replace(text_input, \"\")\n",
    "    \n",
    "print(colored(text_input, \"red\") + colored(continuation, \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1393f7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' attacking appearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_category(continuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078eec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
